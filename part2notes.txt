PART 2, AB TESTING THEORY

//timestamp: 1:59:39

AB testing helps data scienists to measure the effectiveness
changes to a product to rely on data evidence rather than
intuition for more product changes.

It also helps them to identify the most effective changes of
a product bc it allows people to optimize the product.

We can validate what changes can improve a product before the
changes are made with supported evidence from AB testing.

AB testing is a very popular way for businesses to test out:
- UX features
- new versions of a product
- new versions of an algorithm

before launching.

You can do AB testing by showing the current version to a CONTROL
group, and the new version to an EXPERIMENTAL group.




------TESTING PROCESS STEPS
1. State the hypothesis of the A/B test.
- Coming up with a hypothesis. Includes understanding how you'll measure the success,
which is called the primary metric.
2. A/B Test design, power analysis
- Design entire test. Making assumptions about the most important parameters
of the test, calculating minimum sample size required to claim statistical
significance.

//timestamp: 2:04:27

3. Run the A/B test.
- Make sure test runs smoothly. Make sure parameters are satisfied.
Collecting data.
4. Results analysis -> statistical significance
- Testing the hypothesis in step 1 by running analysis on the data
collected in step 3.
5. Results analysis -> practical significance
- Seeing if the conclusions made can make things happen in the real world.




---NUMBER ONE: HYPOTHESIS AND PRIMARY METRIC

//timestamp: 2:06:25

1a. Creating a business hypothesis
Describes the two products being compared and what the desired
impact is for the business.
"Using the color green for our website will increase engagment"

1b. Selecting a primary metric.
There should only be one primary metric in an A/B test. It is used to
define a statistically significant difference between the two things.
Revenue isn't always the end goal, so you must look to tie a metric
to a relevant thing.

//timestamp: 2:08:44

Metric Validity question - If the chosen metric were to increase significantly
while everything else stays constant, would we achieve our goal and address
the problem?

If our end goal is revenue, then our primary metric would be conversion rate.
Conversion rate = (number of conversions / number of total visitors) * 100%

If our end goal is engagment, then our formula is:

CTR = (Number of clicks / Number of impressions) x 100%
This is the click through rate.
This metric is good for email marketing, paid advertisements, and the like.

//timestamp: 2:11:47

//timestamp: 2:13:46

Statistical hypothesis is a procedure used to determine whether there is a
difference between the observed and expected data.
To decide how to fix a potential issue in a product.

We can have two types... null hypothesis and alternative hypothesis.
They mean a hyp that we don't want to happen, and one we want to happen,
respectively.

Example:
Null -> CTR of a button being blue is the same as being green.
Alternative -> CTR of button being green is larger than blue.

We want to reject the null and accept the alternative to improve our product.

We are ready to proceed to the next stage in the A/B testing process.
//timestamp: 2:16:14

------NUMBER TWO: A/B TEST DESIGN
We need to perform the power analysis and discover the minimum
sample size in order to design our test.

Three steps:
1. Power analysis - making assumptions about the model parameters
    The power is the probability of rejecting a null hypothesis.
    It is the probability of not making a type II error (not rejecting a null hyp)
    Usually the power of an A/B test is 80%. This means there is a 20% allowable
    chance to not reject a null hypothesis.

        //timestamp: 2:18:53

    **Significance level - The probability of rejecting the null while the null is true.
    This is like a false positive. This is a type one error. It is common to pick
    5% as the significance level of a test. As an inverse, having these 5% stat means
    we have a confidence level of 95% that there is a good difference between the control
    and experiemental group.

    ** Minimum detectable effect - Basically what is the minimum amount of change to
    warrant having a call to action about this new change? In other words, the smallest effect that
    matters. The amount depends from business to business. This value will translate statistical
    significance to practical significance.

    //timestamp: 2:20:38
    
2. Minimum sample size - using the parameters from power analysis
  To make sure that our results are repeatable, robust, and generalized to the
  entire population,

  //timestamp: 2:22:27
3. Test duration - yep.
