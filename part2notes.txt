PART 2, AB TESTING THEORY

//timestamp: 1:59:39

AB testing helps data scienists to measure the effectiveness
changes to a product to rely on data evidence rather than
intuition for more product changes.

It also helps them to identify the most effective changes of
a product bc it allows people to optimize the product.

We can validate what changes can improve a product before the
changes are made with supported evidence from AB testing.

AB testing is a very popular way for businesses to test out:
- UX features
- new versions of a product
- new versions of an algorithm

before launching.

You can do AB testing by showing the current version to a CONTROL
group, and the new version to an EXPERIMENTAL group.




------TESTING PROCESS STEPS
1. State the hypothesis of the A/B test.
- Coming up with a hypothesis. Includes understanding how you'll measure the success,
which is called the primary metric.
2. A/B Test design, power analysis
- Design entire test. Making assumptions about the most important parameters
of the test, calculating minimum sample size required to claim statistical
significance.

//timestamp: 2:04:27

3. Run the A/B test.
- Make sure test runs smoothly. Make sure parameters are satisfied.
Collecting data.
4. Results analysis -> statistical significance
- Testing the hypothesis in step 1 by running analysis on the data
collected in step 3.
5. Results analysis -> practical significance
- Seeing if the conclusions made can make things happen in the real world.




---NUMBER ONE: HYPOTHESIS AND PRIMARY METRIC

//timestamp: 2:06:25

1a. Creating a business hypothesis
Describes the two products being compared and what the desired
impact is for the business.
"Using the color green for our website will increase engagment"

1b. Selecting a primary metric.
There should only be one primary metric in an A/B test. It is used to
define a statistically significant difference between the two things.
Revenue isn't always the end goal, so you must look to tie a metric
to a relevant thing.

//timestamp: 2:08:44

Metric Validity question - If the chosen metric were to increase significantly
while everything else stays constant, would we achieve our goal and address
the problem?

If our end goal is revenue, then our primary metric would be conversion rate.
Conversion rate = (number of conversions / number of total visitors) * 100%

If our end goal is engagment, then our formula is:

CTR = (Number of clicks / Number of impressions) x 100%
This is the click through rate.
This metric is good for email marketing, paid advertisements, and the like.

//timestamp: 2:11:47

//timestamp: 2:13:46

Statistical hypothesis is a procedure used to determine whether there is a
difference between the observed and expected data.
To decide how to fix a potential issue in a product.

We can have two types... null hypothesis and alternative hypothesis.
They mean a hyp that we don't want to happen, and one we want to happen,
respectively.

Example:
Null -> CTR of a button being blue is the same as being green.
Alternative -> CTR of button being green is larger than blue.

We want to reject the null and accept the alternative to improve our product.

We are ready to proceed to the next stage in the A/B testing process.
//timestamp: 2:16:14

------NUMBER TWO: A/B TEST DESIGN
We need to perform the power analysis and discover the minimum
sample size in order to design our test.

Three steps:
1. Power analysis - making assumptions about the model parameters
    The power is the probability of rejecting a null hypothesis.
    It is the probability of not making a type II error (not rejecting a null hyp)
    Usually the power of an A/B test is 80%. This means there is a 20% allowable
    chance to not reject a null hypothesis.

        //timestamp: 2:18:53

    **Significance level - The probability of rejecting the null while the null is true.
    This is like a false positive. This is a type one error. It is common to pick
    5% as the significance level of a test. As an inverse, having these 5% stat means
    we have a confidence level of 95% that there is a good difference between the control
    and experiemental group.

    ** Minimum detectable effect - Basically what is the minimum amount of change to
    warrant having a call to action about this new change? In other words, the smallest effect that
    matters. The amount depends from business to business. This value will translate statistical
    significance to practical significance.

    //timestamp: 2:20:38
    
2. Minimum sample size - using the parameters from power analysis
  To make sure that our results are repeatable, robust, and generalized to the
  entire population, we need to avoid p-hacking (which is the inappropriate manipulation of data 
  analysis to enable a favored result to be presented as statistically significant.)
  We want to make sure we collect the enough amount of observations for a minimum. 
  We must determine the sample sizes for the control and experiemental groups.

  To do this, we need to take a closer look at our primary metric. There are two scenarios:

  2a. primary metric of A/B testing is a binary variable
    - Conversion or no conversion.
  2b. primary metric of A/B testing is in the form of proportions or averages.
    - Mean click through rate

  //timestamp: 2:25:18

  Let's do an example. 

  A null hypothesis (Ho) can be the control and experimental primary metrics equalling the same
  number. (This is bad because there is no difference).
  An alternative hypothesis can be them not being equal. (H1)

  If we take a large enough sample size from a population, it can be a normal
  distribution even if the population wasn't a normal distribution.

  There is a long equation and derivation for the equation to the minimum sample
  size. It isn't necessary to memorize this equation as there are online calculators
  that ask you for everything.
  //timestamp: 2:28:50

  //timestamp: 2:29:24

  Note that some calculators will confuse statistical significance with
  confidence level. Confidence level is what is at 90-99%, and statistical
  significance is what is at 5%. Sometimes calculators put the significance
  level at 95%, meaning the opposite accidentally.

  //timestamp: 2:22:27
3. Test duration - yep.
  This needs to be figured out before running the experiment. Some people stop
  the test when they discover statistical significance (which is p-hacking). 
  Duration can be equal to N over # of vistors per day. Written like this:

  * N is minimum sample size
  * # of visitors per day is average # of visitors on your site a day.

  Duration = (N) / (# visitors per day) 

  //timestamp: 2:31:08

  Be sure you use common sense when running your test duration. Don't
  run a month long test during December, when there is Christmas
  and the end of the year. This might mess up the results because
  things could be different on these days. A crucial part of testing is
  to have things be random.
  //timestamp: 2:32:34
