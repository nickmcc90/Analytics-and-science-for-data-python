first init

Firstly, we are going to learn how to load in different files. Like csv and excel files.





---LOADING DATA---
Goal: How to load different sources of data using python


1. First we import pandas as pd
2. Have a variable that reads the data. Like this:

data_csv = pd.read_csv('filename.csv')

If we print(data_csv), it'll print out all the data.

3. We can also load in .txt files. 

data_txt = pd.read_txt('textfile', header = 0, sep = ',')

The first argument is the file, second arg is the line where the file starts such that we don't
include that header in the data (but we include it in the column headers), and sep is the symbol 
that separates the rows inside the .txt file.

4. We can load excel files too.

data_excel = pd.read_excel('file.xlsx', sheet_name='Sheet1')

First arg is the excel filename, second arg is the page within the file you want to read in.

5. We can load json files too.

data_json = pd.read_json('file.json')

Explanatory.

6. We can load in data from SQL databases.

- Import the library: import sqlite3
- Connect to the db: connection_db = sqlite3.connect("database_name.db")
- Now we can specify a query. Like this:

query_1 = 'SELECT col_1 FROM table_name'

timestamp 21:41

7. In order to carry out this query_1, we need to do this:

**The first arg is the query and the second arg is the database.
data_sql = pd.read_sql(query_1, connection_db)







---DATA EXPLORATION AND PREPROCESSING ---
Goal: How to explore the data and preprocess it. How to fill
in missing values, how to access different values, etc.


1. The following is a way to have the first 5 rows of an entire data set with all
of the columns.

data_csv = pd.read_csv('filename.csv')
print(data_csv.head())

# The head function does this.

We can show the first 100 rows with putting the number into the head.
Like this:

print(data_csv.head(100))

2. The following is a way to have the last 5 rows of an entire data set with all
of the columns.

print(data_csv.tail())

# tail does the bottom of the table.

3. The following is a way to gather more information about the types of data
in the table, along with the amount of non-null values in the set.

print(data_csv.info())

# This shows the column titles, the non-null content, and the data types.


4. If we want to drop all the null values in the data set, we can do the following.

print(data_csv.dropna())


5. If you want to fill the N/A values with a certain value instead of dropping them, do this.

print(data_csv.fillna("NULL"))


6. Let's say you have rows in the data that are identical to each other and want to get rid
of them.

print(data_csv.drop_duplicates())

7. How can we access certain rows in the database? Like this:
We want the 10th row.

print(data_csv.iloc[10])

** How can we access certains rows and columns with .loc?

print(data_csv.loc['X'])  # This is to get a certain row

print(data_csv.loc[:, "A2"])  # This is to get a certain column. We need to specify all rows here.

--
iloc is used to access integer labeled rows
loc is used to access string labeled rows and columns






-- DATA AGGREGATION FILTERING, SORTING, GROUPING---
Goal: How to manipulate data to our liking such that we
can make our inferences to it.

Grouping - combining data by like variables.
Filtering - taking out data points based on criteria
Sorting - organize info in ascending or descending order.


// timestamp: 41:59

** data is a table of data with the columns name, age, salary, department

1. Let's say we want to sort the data by a specific column.
data.sort_values()

// timestamp: 43:56

1a. sort_values...
1b. by = column we want to organize by.
1c. ascending = True (we could've skipped this argument.)
1d. The default value is set as ascending True.
data = {}
data.sort_values(by = 'Salary', ascending=True)

// timestamp: 44:44

2. We can use the groupby function to further assess the data.
Look at these functions...

# this is the number of times each department appears for the other columns
data.groupby("Department").count()
# and this is how many times each department appears for the name column only
data.groupby("Department")["Name"].count()

# let's grab the average salary per department
data.groupby("Department")["Salary"].mean()

timestamp -> 55:37